{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'backend:cudaMallocAsync'\n",
    "\n",
    "print(os.environ.get('PYTORCH_CUDA_ALLOC_CONF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the best available device for PyTorch.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "        # The flag below controls whether to allow TF32 on matmul.\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        # The flag below controls whether to allow TF32 on cuDNN.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        # Print GPU info\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Set up GPU memory management\n",
    "        memory_limit_mb = 4095.5  # Adjust as needed\n",
    "\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "        memory_limit = memory_limit_mb * 1024 ** 2\n",
    "        memory_fraction = memory_limit / total_memory\n",
    "\n",
    "        torch.cuda.set_per_process_memory_fraction(memory_fraction, device=0)\n",
    "\n",
    "        print(f\"Set GPU memory fraction to {memory_fraction:.2%}\")\n",
    "\n",
    "        # Ensure memory is allocated\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        print(\"Using Apple Silicon MPS device\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"Using CPU device\")\n",
    "    \n",
    "    return torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory = True if device.type == 'cuda' else False\n",
    "pin_memory_device = 'cuda' if device.type == 'cuda' else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load & transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create transforms without normalization to calculate dataset statistics\n",
    "initial_transforms = v2.Compose([\n",
    "    v2.Resize((480, 270)),# changed from v1\n",
    "    v2.Grayscale(num_output_channels=1),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary dataset to calculate mean and std\n",
    "temp_dataset = ImageFolder(root='../data/new_pool', transform=initial_transforms)\n",
    "\n",
    "temp_loader = DataLoader(temp_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_stats = True\n",
    "\n",
    "if calc_stats:\n",
    "    print(f\"Computing dataset statistics using device: {device}\")\n",
    "    print(f\"Number of images to process: {len(temp_dataset)}\")\n",
    "\n",
    "    channels_sum = torch.zeros(3, device=device)\n",
    "    channels_sqrd_sum = torch.zeros(3, device=device)\n",
    "    num_batches = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(temp_loader, desc=\"Computing mean/std\")):\n",
    "        data = data.to(device, non_blocking=True)  # Add non_blocking=True\n",
    "        with autocast(device.type):\n",
    "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "            channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            print(f\"\\nProcessed {batch_idx * temp_loader.batch_size} images in {batch_time:.2f}s\")\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = torch.sqrt((channels_sqrd_sum / num_batches) - (mean ** 2))\n",
    "\n",
    "    mean = mean.cpu().tolist()\n",
    "    std = std.cpu().tolist()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Dataset mean: {mean}\")\n",
    "    print(f\"Dataset std: {std}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Release memory\n",
    "    del temp_dataset, temp_loader, channels_sum, channels_sqrd_sum, data\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    mean=[0.35439860820770264, 0.35439860820770264, 0.35439860820770264]\n",
    "    std=[0.20184797048568726, 0.20184797048568726, 0.20184797048568726]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.Resize((480, 270)), # changed from v1\n",
    "    v2.RandomVerticalFlip(p=0.2),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=1.25, p=1.0),  # Subtle sharpness changes\n",
    "    # Small perspective changes to simulate different card angles\n",
    "    v2.RandomPerspective(\n",
    "        distortion_scale=0.1,  # Keep perspective changes minimal\n",
    "        p=0.1                   # Apply only 30% of the time\n",
    "    ),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((480, 270)),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=1.25, p=1.0),  # Subtle sharpness changes\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Partition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with appropriate transforms\n",
    "full_dataset = ImageFolder(root='../data/new_pool', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split ratios\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.20\n",
    "\n",
    "# calculate lengths\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test datasets with appropriate transforms\n",
    "train_data, val_data, test_data = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly override transforms\n",
    "train_data.dataset.transform = train_transforms\n",
    "val_data.dataset.transform = val_transforms \n",
    "test_data.dataset.transform = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)  \n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=6, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images(dataset, num_images=9, cols=6):\n",
    "    \"\"\"Display original and transformed images side by side\"\"\"\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    for i in range(num_images):\n",
    "        # Get an image and its label\n",
    "        img_path = dataset.samples[i][0]\n",
    "        original_img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms multiple times to see variability\n",
    "        transformed_img = train_transforms(original_img)\n",
    "        \n",
    "        # Convert tensor to image for display\n",
    "        transformed_img = transformed_img.permute(1, 2, 0)  # Change from CxHxW to HxWxC\n",
    "        transformed_img = transformed_img * torch.tensor(std) + torch.tensor(mean)  # Denormalize\n",
    "        transformed_img = torch.clamp(transformed_img, 0, 1)  # Clamp values to valid range\n",
    "        \n",
    "        # Plot original and transformed\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Original')\n",
    "            \n",
    "        plt.subplot(2, num_images, i + 1 + num_images)\n",
    "        plt.imshow(transformed_img)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Transformed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Display sample images\n",
    "show_transformed_images(full_dataset)\n",
    "\n",
    "# Show multiple augmentations of same image\n",
    "def show_multiple_augmentations(dataset, img_index=0, num_augments=5):\n",
    "    \"\"\"Show multiple augmentations of the same image\"\"\"\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    \n",
    "    # Get original image\n",
    "    img_path = dataset.samples[img_index][0]\n",
    "    original_img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Show original\n",
    "    plt.subplot(1, num_augments + 1, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Original')\n",
    "    \n",
    "    # Show different augmentations\n",
    "    for i in range(num_augments):\n",
    "        transformed_img = train_transforms(original_img)\n",
    "        transformed_img = transformed_img.permute(1, 2, 0)\n",
    "        transformed_img = transformed_img * torch.tensor(std) + torch.tensor(mean)\n",
    "        transformed_img = torch.clamp(transformed_img, 0, 1)\n",
    "        \n",
    "        plt.subplot(1, num_augments + 1, i + 2)\n",
    "        plt.imshow(transformed_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Aug {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_random_augmentations(dataset, num_images=5):\n",
    "    \"\"\"Display random images before and after augmentation\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(dataset))[:num_images]\n",
    "    \n",
    "    for idx, i in enumerate(indices):\n",
    "        # Get an image and its label\n",
    "        img_path = dataset.samples[i][0]\n",
    "        original_img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        transformed_img = train_transforms(original_img)\n",
    "        \n",
    "        # Denormalize transformed image\n",
    "        transformed_img = transformed_img.permute(1, 2, 0)\n",
    "        transformed_img = transformed_img * torch.tensor(std) + torch.tensor(mean)\n",
    "        transformed_img = torch.clamp(transformed_img, 0, 1)\n",
    "        \n",
    "        # Plot original\n",
    "        plt.subplot(2, num_images, idx + 1)\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(f'Original {dataset.classes[dataset.targets[i]]}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot transformed\n",
    "        plt.subplot(2, num_images, idx + 1 + num_images)\n",
    "        plt.imshow(transformed_img)\n",
    "        plt.title('Augmented')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show random images and their augmentations\n",
    "print(\"Random samples from dataset with augmentations:\")\n",
    "show_random_augmentations(full_dataset)\n",
    "\n",
    "# Show multiple augmentations of the same random image\n",
    "rand_idx = torch.randint(0, len(train_data), (1,)).item()\n",
    "print(f\"\\nMultiple augmentations of a random {full_dataset.classes[full_dataset.targets[rand_idx]]} card:\")\n",
    "show_multiple_augmentations(full_dataset, img_index=rand_idx, num_augments=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape constants (changed from v1)\n",
    "IMG_WIDTH = 480 \n",
    "IMG_HEIGHT = 270\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnoSymbolClassifier(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 16, 3, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(16, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()        \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * (IMG_HEIGHT // 8) * (IMG_WIDTH // 8 ), 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 15)  # Assuming 15 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.utils.data.Dataset:\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnoSymbolClassifier()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(32, IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device, non_blocking=True)\n",
    "model = torch.compile(model, backend=\"inductor\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimising model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 5e-3\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Optimizer & cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    scaler = GradScaler()\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    training_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)  # Move data to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device.type):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        training_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float32).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_item = loss.item()\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss_item:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    avg_loss = training_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Define validate & test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)  # Move data to device\n",
    "\n",
    "            with autocast(device.type):\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float32).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    print(f\"Avg loss: {avg_loss:>8f}, Accuracy: {(100*accuracy):>0.1f}%\\n\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Define overfitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overfitting(train_loss, val_loss, train_acc, val_acc, threshold=0.1):\n",
    "    loss_gap = abs(train_loss - val_loss)\n",
    "    acc_gap = abs(train_acc - val_acc)\n",
    "    \n",
    "    is_overfitting = (loss_gap > threshold) and (train_acc > val_acc + threshold)\n",
    "    \n",
    "    if is_overfitting:\n",
    "        print(f\"Warning: Possible overfitting detected\")\n",
    "        print(f\"Loss gap: {loss_gap:.4f}, Accuracy gap: {acc_gap:.4f}\")\n",
    "    \n",
    "    return is_overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "epoch_times = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_metrics = None\n",
    "stopped_early = False\n",
    "is_overfitting = 0\n",
    "patience = 8  # Number of epochs with no improvement after which training will be stopped\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_loss, train_accuracy = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    scheduler.step(train_loss)  \n",
    "        \n",
    "    val_loss, val_accuracy = test_loop(val_loader, model, loss_fn)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Increment overfitting counter if overfitting detected\n",
    "    if check_overfitting(train_loss, val_loss, train_accuracy, val_accuracy):\n",
    "        is_overfitting += 1\n",
    "    else:\n",
    "        is_overfitting = 0\n",
    "\n",
    "    # Check all conditions\n",
    "    accuracy_gap = abs(train_accuracy - val_accuracy)\n",
    "    conditions_met = (\n",
    "        train_loss >= 0.1 and\n",
    "        val_loss >= 0.1 and\n",
    "        train_accuracy <= 0.97 and\n",
    "        val_accuracy <= 0.97 and\n",
    "        accuracy_gap <= 0.05\n",
    "    )\n",
    "\n",
    "    # Save model if conditions are met and validation loss improved\n",
    "    if conditions_met and val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        torch.save(model.state_dict(), '../data/models/best_symbol_classifier.pth')\n",
    "\n",
    "    # Stop if overfitting persists for multiple epochs\n",
    "    if is_overfitting >= patience:\n",
    "        print(f\"Early stopping triggered due to persistent overfitting.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "total_training_time = time.time() - total_start_time\n",
    "\n",
    "# Save both models\n",
    "torch.save(model.state_dict(), '../data/models/full_symbol_classifier.pth')\n",
    "\n",
    "if best_model_metrics:\n",
    "    print(\"\\nBest model saved with metrics:\")\n",
    "    for key, value in best_model_metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nTraining complete in {total_training_time:.2f} seconds\")\n",
    "print(\"\\n-------------------------------\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, len(train_losses) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Epoch duration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, epoch_times, label='Time per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Time Taken per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Create & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = UnoSymbolClassifier()\n",
    "model.to(device, non_blocking=True)\n",
    "model = torch.compile(model, backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_early = False\n",
    "\n",
    "# Load the best model (if saved during early stopping)\n",
    "if stopped_early:\n",
    "    model.load_state_dict(torch.load('../data/models/best_symbol_classifier.pth'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('../data/models/full_symbol_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Test model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dataset.transform = val_transforms\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"Test Results on the Test Set:\")\n",
    "test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Test model on own image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, inspect the first layer\n",
    "first_layer = next(model.children())\n",
    "print(f\"First layer expects input shape: {first_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = v2.Compose([\n",
    "    v2.Resize((480, 270)),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_image(image_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed_image = image_transform(image)\n",
    "    \n",
    "    # Convert back to displayable format\n",
    "    display_image = transformed_image.clone()\n",
    "    display_image = display_image * torch.tensor(std).view(3,1,1) + torch.tensor(mean).view(3,1,1)\n",
    "    display_image = torch.clamp(display_image, 0, 1)\n",
    "    display_image = display_image.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Display grayscale image\n",
    "    plt.imshow(display_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = '../data/test_images/green_draw2.jpg'  # Replace with your image path\n",
    "input_tensor = load_and_display_image(image_path)\n",
    "input_tensor = input_tensor.unsqueeze(0).to(device, non_blocking=True)  # Add batch dimension and move to device\n",
    "print(f\"Transformed input tensor shape: {input_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '../data/test_images/green_draw2.jpg' \n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "input_tensor = image_transform(original_image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "input_tensor = input_tensor.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_image(image_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed_image = image_transform(image)\n",
    "    \n",
    "    # Prepare for display\n",
    "    display_image = transformed_image.clone()\n",
    "    display_image = display_image * torch.tensor(std).view(3,1,1) + torch.tensor(mean).view(3,1,1)\n",
    "    display_image = torch.clamp(display_image, 0, 1)\n",
    "    display_image = display_image.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Display image\n",
    "    plt.imshow(display_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    input_tensor = transformed_image.unsqueeze(0).to(device)\n",
    "    return input_tensor\n",
    "\n",
    "def predict_image_class(model, input_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(device.type):\n",
    "            output = model(input_tensor)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "image_path = '../data/test_images/green_draw2.jpg'  # Replace with your image path\n",
    "input_tensor = load_and_prepare_image(image_path)\n",
    "predicted_class = predict_image_class(model, input_tensor)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display transformed image\n",
    "transformed_image = input_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Display transformed image\n",
    "transformed_image = input_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
    "# Undo normalization for display\n",
    "transformed_image = transformed_image * np.array(mean) + np.array(std)\n",
    "transformed_image = np.clip(transformed_image, 0, 1)\n",
    "\n",
    "plt.imshow(input_tensor[1)\n",
    "plt.title('Transformed Image (3 channels)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Print channel information\n",
    "print(f\"Transformed image shape: {transformed_image.shape}\")\n",
    "print(f\"Are all channels identical? {np.allclose(transformed_image[:,:,0], transformed_image[:,:,1])}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    with autocast(device.type):  # Add autocast here for consistent mixed precision inference\n",
    "        output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = full_dataset.classes  # Replace with your actual dataset variable if different\n",
    "print(f\"Predicted class index: {predicted_class}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Map predicted index to class name\n",
    "if predicted_class < len(class_names):\n",
    "    predicted_label = class_names[predicted_class]\n",
    "    print(f\"Predicted class: {predicted_label}\")\n",
    "else:\n",
    "    print(\"Predicted class index is out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
