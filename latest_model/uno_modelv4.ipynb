{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend:cudaMallocAsync\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'backend:cudaMallocAsync'\n",
    "\n",
    "print(os.environ.get('PYTORCH_CUDA_ALLOC_CONF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the best available device for PyTorch.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "        # The flag below controls whether to allow TF32 on matmul.\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        # The flag below controls whether to allow TF32 on cuDNN.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        # Print GPU info\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Set up GPU memory management\n",
    "        memory_limit_mb = 4095.5  # Adjust as needed\n",
    "\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "        memory_limit = memory_limit_mb * 1024 ** 2\n",
    "        memory_fraction = memory_limit / total_memory\n",
    "\n",
    "        torch.cuda.set_per_process_memory_fraction(memory_fraction, device=0)\n",
    "\n",
    "        print(f\"Set GPU memory fraction to {memory_fraction:.2%}\")\n",
    "\n",
    "        # Ensure memory is allocated\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        print(\"Using Apple Silicon MPS device\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"Using CPU device\")\n",
    "    \n",
    "    return torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Set GPU memory fraction to 100.00%\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory = True if device.type == 'cuda' else False\n",
    "pin_memory_device = 'cuda' if device.type == 'cuda' else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load & transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create transforms without normalization to calculate dataset statistics\n",
    "initial_transforms = v2.Compose([\n",
    "    v2.Resize((384, 216)),# changed from v1\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary dataset to calculate mean and std\n",
    "temp_dataset = ImageFolder(root='../data/merged_pool', transform=initial_transforms)\n",
    "\n",
    "temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=False, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_stats = False\n",
    "\n",
    "if calc_stats:\n",
    "    print(f\"Computing dataset statistics using device: {device}\")\n",
    "    print(f\"Number of images to process: {len(temp_dataset)}\")\n",
    "\n",
    "    channels_sum = torch.zeros(3, device=device)\n",
    "    channels_sqrd_sum = torch.zeros(3, device=device)\n",
    "    num_batches = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(temp_loader, desc=\"Computing mean/std\")):\n",
    "        data = data.to(device, non_blocking=True)  # Add non_blocking=True\n",
    "        with autocast(device.type):\n",
    "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "            channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            print(f\"\\nProcessed {batch_idx * temp_loader.batch_size} images in {batch_time:.2f}s\")\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = torch.sqrt((channels_sqrd_sum / num_batches) - (mean ** 2))\n",
    "\n",
    "    mean = mean.cpu().tolist()\n",
    "    std = std.cpu().tolist()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Dataset mean: {mean}\")\n",
    "    print(f\"Dataset std: {std}\")\n",
    "\n",
    "    # Release memory\n",
    "    del temp_dataset, temp_loader, channels_sum, channels_sqrd_sum, data\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    mean=[0.5479778051376343, 0.526210367679596, 0.4944702088832855]\n",
    "    std=[0.2237844169139862, 0.23763211071491241, 0.26044926047325134]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.Resize((384, 216)), # changed from v1\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((384, 216)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Partition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with appropriate transforms\n",
    "train_data = ImageFolder(root='../data/merged_pool', transform=train_transforms)\n",
    "\n",
    "val_data = ImageFolder(root='../data/validation_pool', transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=64, num_workers=6, pin_memory=pin_memory, pin_memory_device=pin_memory_device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape constants (changed from v1)\n",
    "IMG_WIDTH = 384\n",
    "IMG_HEIGHT = 216\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnoSymbolClassifier(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 1, 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 16, 3, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 1, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.conv2_block = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 1, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(16, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.conv3_block = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.conv4_block = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(128 * (IMG_HEIGHT // 16) * (IMG_WIDTH // 16), 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 54)\n",
    "\n",
    "    def forward(self, x) -> torch.utils.data.Dataset:\n",
    "        x = self.conv1_block(x)\n",
    "        x = self.conv2_block(x)\n",
    "        x = self.conv3_block(x)\n",
    "        x = self.conv4_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnoSymbolClassifier(\n",
       "  (conv1_block): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2_block): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3_block): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4_block): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=39936, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fc4): Linear(in_features=64, out_features=54, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UnoSymbolClassifier()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UnoSymbolClassifier                      [64, 54]                  --\n",
       "├─Sequential: 1-1                        [64, 16, 108, 192]        --\n",
       "│    └─Conv2d: 2-1                       [64, 8, 216, 384]         32\n",
       "│    └─BatchNorm2d: 2-2                  [64, 8, 216, 384]         16\n",
       "│    └─ReLU: 2-3                         [64, 8, 216, 384]         --\n",
       "│    └─ReflectionPad2d: 2-4              [64, 8, 218, 386]         --\n",
       "│    └─Conv2d: 2-5                       [64, 16, 216, 384]        1,168\n",
       "│    └─BatchNorm2d: 2-6                  [64, 16, 216, 384]        32\n",
       "│    └─ReLU: 2-7                         [64, 16, 216, 384]        --\n",
       "│    └─Conv2d: 2-8                       [64, 16, 216, 384]        272\n",
       "│    └─BatchNorm2d: 2-9                  [64, 16, 216, 384]        32\n",
       "│    └─ReLU: 2-10                        [64, 16, 216, 384]        --\n",
       "│    └─MaxPool2d: 2-11                   [64, 16, 108, 192]        --\n",
       "├─Sequential: 1-2                        [64, 32, 54, 96]          --\n",
       "│    └─Conv2d: 2-12                      [64, 16, 108, 192]        272\n",
       "│    └─BatchNorm2d: 2-13                 [64, 16, 108, 192]        32\n",
       "│    └─ReLU: 2-14                        [64, 16, 108, 192]        --\n",
       "│    └─ReflectionPad2d: 2-15             [64, 16, 110, 194]        --\n",
       "│    └─Conv2d: 2-16                      [64, 32, 108, 192]        4,640\n",
       "│    └─BatchNorm2d: 2-17                 [64, 32, 108, 192]        64\n",
       "│    └─ReLU: 2-18                        [64, 32, 108, 192]        --\n",
       "│    └─Conv2d: 2-19                      [64, 32, 108, 192]        1,056\n",
       "│    └─BatchNorm2d: 2-20                 [64, 32, 108, 192]        64\n",
       "│    └─ReLU: 2-21                        [64, 32, 108, 192]        --\n",
       "│    └─MaxPool2d: 2-22                   [64, 32, 54, 96]          --\n",
       "├─Sequential: 1-3                        [64, 64, 27, 48]          --\n",
       "│    └─Conv2d: 2-23                      [64, 32, 54, 96]          1,056\n",
       "│    └─BatchNorm2d: 2-24                 [64, 32, 54, 96]          64\n",
       "│    └─ReLU: 2-25                        [64, 32, 54, 96]          --\n",
       "│    └─ReflectionPad2d: 2-26             [64, 32, 56, 98]          --\n",
       "│    └─Conv2d: 2-27                      [64, 64, 54, 96]          18,496\n",
       "│    └─BatchNorm2d: 2-28                 [64, 64, 54, 96]          128\n",
       "│    └─ReLU: 2-29                        [64, 64, 54, 96]          --\n",
       "│    └─Conv2d: 2-30                      [64, 64, 54, 96]          4,160\n",
       "│    └─BatchNorm2d: 2-31                 [64, 64, 54, 96]          128\n",
       "│    └─ReLU: 2-32                        [64, 64, 54, 96]          --\n",
       "│    └─MaxPool2d: 2-33                   [64, 64, 27, 48]          --\n",
       "├─Sequential: 1-4                        [64, 128, 13, 24]         --\n",
       "│    └─Conv2d: 2-34                      [64, 64, 27, 48]          4,160\n",
       "│    └─BatchNorm2d: 2-35                 [64, 64, 27, 48]          128\n",
       "│    └─ReLU: 2-36                        [64, 64, 27, 48]          --\n",
       "│    └─ReflectionPad2d: 2-37             [64, 64, 29, 50]          --\n",
       "│    └─Conv2d: 2-38                      [64, 128, 27, 48]         73,856\n",
       "│    └─BatchNorm2d: 2-39                 [64, 128, 27, 48]         256\n",
       "│    └─ReLU: 2-40                        [64, 128, 27, 48]         --\n",
       "│    └─Conv2d: 2-41                      [64, 128, 27, 48]         16,512\n",
       "│    └─BatchNorm2d: 2-42                 [64, 128, 27, 48]         256\n",
       "│    └─ReLU: 2-43                        [64, 128, 27, 48]         --\n",
       "│    └─MaxPool2d: 2-44                   [64, 128, 13, 24]         --\n",
       "├─Flatten: 1-5                           [64, 39936]               --\n",
       "├─Sequential: 1-6                        [64, 64]                  --\n",
       "│    └─Linear: 2-45                      [64, 64]                  2,555,968\n",
       "│    └─BatchNorm1d: 2-46                 [64, 64]                  128\n",
       "│    └─ReLU: 2-47                        [64, 64]                  --\n",
       "│    └─Dropout: 2-48                     [64, 64]                  --\n",
       "├─Sequential: 1-7                        [64, 128]                 --\n",
       "│    └─Linear: 2-49                      [64, 128]                 8,320\n",
       "│    └─BatchNorm1d: 2-50                 [64, 128]                 256\n",
       "│    └─ReLU: 2-51                        [64, 128]                 --\n",
       "│    └─Dropout: 2-52                     [64, 128]                 --\n",
       "├─Sequential: 1-8                        [64, 64]                  --\n",
       "│    └─Linear: 2-53                      [64, 64]                  8,256\n",
       "│    └─BatchNorm1d: 2-54                 [64, 64]                  128\n",
       "│    └─ReLU: 2-55                        [64, 64]                  --\n",
       "│    └─Dropout: 2-56                     [64, 64]                  --\n",
       "├─Linear: 1-9                            [64, 54]                  3,510\n",
       "==========================================================================================\n",
       "Total params: 2,703,446\n",
       "Trainable params: 2,703,446\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 31.61\n",
       "==========================================================================================\n",
       "Input size (MB): 63.70\n",
       "Forward/backward pass size (MB): 6370.39\n",
       "Params size (MB): 10.81\n",
       "Estimated Total Size (MB): 6444.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(64, IMG_CHANNELS, IMG_HEIGHT, IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device, non_blocking=True)\n",
    "model = torch.compile(model, backend=\"inductor\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimising model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Optimizer & cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    scaler = GradScaler()\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "\n",
    "    training_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)  # Move data to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device.type):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        training_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float32).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_item = loss.item()\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss_item:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    avg_loss = training_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Define validate & test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)  # Move data to device\n",
    "\n",
    "            with autocast(device.type):\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "            test_loss += loss.item() * X.size(0)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float32).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = correct / total\n",
    "    print(f\"Avg loss: {avg_loss:>8f}, Accuracy: {(100*accuracy):>0.1f}%\\n\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Define overfitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overfitting(train_loss, val_loss, train_acc, val_acc, threshold=0.1):\n",
    "    loss_gap = abs(train_loss - val_loss)\n",
    "    acc_gap = abs(train_acc - val_acc)\n",
    "    \n",
    "    is_overfitting = (loss_gap > threshold) and (train_acc > val_acc + threshold)\n",
    "    \n",
    "    if is_overfitting:\n",
    "        print(f\"Warning: Possible overfitting detected\")\n",
    "        print(f\"Loss gap: {loss_gap:.4f}, Accuracy gap: {acc_gap:.4f}\")\n",
    "    \n",
    "    return is_overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "epoch_times = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_metrics = None\n",
    "stopped_early = False\n",
    "is_overfitting = 0\n",
    "patience = 300  # Number of epochs with no improvement after which training will be stopped\n",
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.801422  [    0/12257]\n",
      "loss: 4.524414  [ 6400/12257]\n",
      "Avg loss: 6.788830, Accuracy: 1.9%\n",
      "\n",
      "Epoch 1 completed in 2.02 minutes\n",
      "Training Loss: 4.5649, Training Accuracy: 0.0021\n",
      "Validation Loss: 6.7888, Validation Accuracy: 0.0185\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.592712  [    0/12257]\n",
      "loss: 4.190369  [ 6400/12257]\n",
      "Avg loss: 4.153726, Accuracy: 0.5%\n",
      "\n",
      "Epoch 2 completed in 1.08 minutes\n",
      "Training Loss: 4.1385, Training Accuracy: 0.0104\n",
      "Validation Loss: 4.1537, Validation Accuracy: 0.0048\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.822449  [    0/12257]\n",
      "loss: 4.113220  [ 6400/12257]\n",
      "Avg loss: 3.991206, Accuracy: 3.6%\n",
      "\n",
      "Epoch 3 completed in 1.04 minutes\n",
      "Training Loss: 4.0895, Training Accuracy: 0.0106\n",
      "Validation Loss: 3.9912, Validation Accuracy: 0.0364\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.931396  [    0/12257]\n",
      "loss: 4.038422  [ 6400/12257]\n",
      "Avg loss: 3.950435, Accuracy: 2.9%\n",
      "\n",
      "Epoch 4 completed in 1.11 minutes\n",
      "Training Loss: 4.0520, Training Accuracy: 0.0068\n",
      "Validation Loss: 3.9504, Validation Accuracy: 0.0295\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 4.005402  [    0/12257]\n",
      "loss: 4.043213  [ 6400/12257]\n",
      "Avg loss: 3.956122, Accuracy: 3.9%\n",
      "\n",
      "Epoch 5 completed in 1.13 minutes\n",
      "Training Loss: 4.0460, Training Accuracy: 0.0054\n",
      "Validation Loss: 3.9561, Validation Accuracy: 0.0391\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.031281  [    0/12257]\n",
      "loss: 4.043945  [ 6400/12257]\n",
      "Avg loss: 3.949745, Accuracy: 3.4%\n",
      "\n",
      "Epoch 6 completed in 1.12 minutes\n",
      "Training Loss: 4.0271, Training Accuracy: 0.0087\n",
      "Validation Loss: 3.9497, Validation Accuracy: 0.0336\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 4.045929  [    0/12257]\n",
      "loss: 4.093872  [ 6400/12257]\n",
      "Avg loss: 4.033838, Accuracy: 1.4%\n",
      "\n",
      "Epoch 7 completed in 1.11 minutes\n",
      "Training Loss: 4.0199, Training Accuracy: 0.0103\n",
      "Validation Loss: 4.0338, Validation Accuracy: 0.0137\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 4.012543  [    0/12257]\n",
      "loss: 3.980347  [ 6400/12257]\n",
      "Avg loss: 4.118088, Accuracy: 1.3%\n",
      "\n",
      "Epoch 8 completed in 1.04 minutes\n",
      "Training Loss: 4.0434, Training Accuracy: 0.0062\n",
      "Validation Loss: 4.1181, Validation Accuracy: 0.0130\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.082397  [    0/12257]\n",
      "loss: 4.009186  [ 6400/12257]\n",
      "Avg loss: 3.954355, Accuracy: 3.6%\n",
      "\n",
      "Epoch 9 completed in 1.01 minutes\n",
      "Training Loss: 4.0171, Training Accuracy: 0.0051\n",
      "Validation Loss: 3.9544, Validation Accuracy: 0.0364\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 4.079407  [    0/12257]\n",
      "loss: 3.992096  [ 6400/12257]\n",
      "Avg loss: 4.001116, Accuracy: 2.5%\n",
      "\n",
      "Epoch 10 completed in 1.03 minutes\n",
      "Training Loss: 4.0267, Training Accuracy: 0.0032\n",
      "Validation Loss: 4.0011, Validation Accuracy: 0.0254\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 4.235748  [    0/12257]\n",
      "loss: 4.065643  [ 6400/12257]\n",
      "Avg loss: 3.996135, Accuracy: 2.5%\n",
      "\n",
      "Epoch 11 completed in 1.02 minutes\n",
      "Training Loss: 4.0161, Training Accuracy: 0.0026\n",
      "Validation Loss: 3.9961, Validation Accuracy: 0.0247\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.137878  [    0/12257]\n",
      "loss: 4.084839  [ 6400/12257]\n",
      "Avg loss: 3.991506, Accuracy: 1.8%\n",
      "\n",
      "Epoch 12 completed in 1.01 minutes\n",
      "Training Loss: 4.0136, Training Accuracy: 0.0042\n",
      "Validation Loss: 3.9915, Validation Accuracy: 0.0178\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 4.111420  [    0/12257]\n",
      "loss: 4.080505  [ 6400/12257]\n",
      "Avg loss: 3.979672, Accuracy: 2.7%\n",
      "\n",
      "Epoch 13 completed in 1.01 minutes\n",
      "Training Loss: 4.0122, Training Accuracy: 0.0053\n",
      "Validation Loss: 3.9797, Validation Accuracy: 0.0267\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 4.175476  [    0/12257]\n",
      "loss: 4.081665  [ 6400/12257]\n",
      "Avg loss: 3.903177, Accuracy: 3.9%\n",
      "\n",
      "Epoch 14 completed in 1.02 minutes\n",
      "Training Loss: 4.0043, Training Accuracy: 0.0071\n",
      "Validation Loss: 3.9032, Validation Accuracy: 0.0391\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 4.138519  [    0/12257]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_loss, train_accuracy = train_loop(\n",
    "        train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    val_loss, val_accuracy = test_loop(val_loader, model, loss_fn)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    epoch_time_min = epoch_time / 60\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time_min:.2f} minutes\")\n",
    "    print(\n",
    "        f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(\n",
    "        f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Increment overfitting counter if overfitting detected\n",
    "    if check_overfitting(train_loss, val_loss, train_accuracy, val_accuracy):\n",
    "        is_overfitting += 1\n",
    "    else:\n",
    "        is_overfitting = 0\n",
    "\n",
    "    # Check all conditions\n",
    "    accuracy_gap = abs(train_accuracy - val_accuracy)\n",
    "    conditions_met = (\n",
    "        train_loss >= 0.1 and\n",
    "        val_loss >= 0.1 and\n",
    "        train_accuracy <= 0.98 and\n",
    "        val_accuracy <= 0.98 and\n",
    "        accuracy_gap <= 0.07\n",
    "    )\n",
    "\n",
    "    # Save model if conditions are met and validation loss improved\n",
    "    if conditions_met:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        torch.save(model.state_dict(),\n",
    "                   '../data/models/best_symbol_classifier.pth')\n",
    "\n",
    "    # Stop if overfitting persists for multiple epochs\n",
    "    if is_overfitting >= patience:\n",
    "        print(f\"Early stopping triggered due to persistent overfitting.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "total_training_time = time.time() - total_start_time\n",
    "avg_epoch_time = sum(epoch_times) / len(epoch_times) / 60\n",
    "total_training_time_min = total_training_time / 60\n",
    "\n",
    "# Save both models\n",
    "torch.save(model.state_dict(), '../data/models/full_symbol_classifier.pth')\n",
    "\n",
    "if best_model_metrics:\n",
    "    print(\"\\nBest model saved with metrics:\")\n",
    "    for key, value in best_model_metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {avg_epoch_time:.2f} minutes\")\n",
    "print(f\"Total training time: {total_training_time_min:.2f} minutes\")\n",
    "print(\"\\n-------------------------------\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, len(train_losses) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Epoch duration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, epoch_times, label='Time per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Time Taken per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Create & load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = UnoSymbolClassifier()\n",
    "model.to(device, non_blocking=True)\n",
    "model = torch.compile(model, backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_early = False\n",
    "\n",
    "# Load the best model (if saved during early stopping)\n",
    "if stopped_early:\n",
    "    model.load_state_dict(torch.load('../data/models/best_symbol_classifier.pth', weights_only=True))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('../data/models/full_symbol_classifier.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = v2.Compose([\n",
    "    v2.Resize((512, 288)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your image\n",
    "image_path = '../data/test_images/green_draw2.jpg'  # Replace with your image path\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image = image_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension and move to device\n",
    "input_tensor = transformed_image.unsqueeze(0).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    _, predicted = torch.max(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class\n",
    "predicted_class = class_names[predicted.item()]\n",
    "print(f'Predicted Class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image and prediction\n",
    "plt.imshow(image)\n",
    "plt.title(f'Predicted Class: {predicted_class}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
