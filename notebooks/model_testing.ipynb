{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow_docs.modeling import EpochDots\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint \n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3800)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/data_pool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory(data_dir, batch_size=32, image_size=(256, 256), color_mode='rgb', shuffle=True)\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(data.class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Get class names\n",
    "print(f\"Class names: {data.class_names}\")\n",
    "\n",
    "# Get total number of files\n",
    "total_files = len(data.file_paths)\n",
    "print(f\"Total number of files: {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x, y: (x / 255.0, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_iterator = data.as_numpy_iterator()\n",
    "batch = scaled_iterator.next()\n",
    "batch[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*0.75)\n",
    "val_size = int(len(data)*0.17)\n",
    "test_size = int(len(data)*0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(train_size)\n",
    "val_data = data.skip(train_size).take(val_size)\n",
    "test_data = data.skip(train_size + val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_size: {train_size}\")\n",
    "print(f\"val_size: {val_size}\")\n",
    "print(f\"test_size: {test_size}\")\n",
    "\n",
    "print(f\"train_data: {len(train_data)}\")\n",
    "print(f\"val_data: {len(val_data)}\")\n",
    "print(f\"test_data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape constants\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# Much smaller L2 regularization\n",
    "reg_factor = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Input Layer\n",
    "    Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),    \n",
    "\n",
    "    # First Convolutional Block - halved filters (16 instead of 32)\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block - halved filters (32 instead of 64)\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third Convolutional Block - halved filters (64 instead of 128)\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "           kernel_regularizer=l2(reg_factor)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten the output and add dense layers\n",
    "    Flatten(),\n",
    "\n",
    "    # Reduced number of dense layers with multiples of 20 nodes\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(reg_factor)),  # 16x number of classes\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  # Reduced dropout \n",
    "\n",
    "    # Reduced number of dense layers with multiples of 20 nodes\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(reg_factor)),  # 16x number of classes\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  # Reduced dropout \n",
    "\n",
    "    # Reduced number of dense layers with multiples of 20 nodes\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(reg_factor)),  # 16x number of classes\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  # Reduced dropout \n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(reg_factor)),  # 8x number of classes\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  # Reduced dropout\n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(reg_factor)),  # 2x number of classes\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(54, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training configuration for longer training\n",
    "training_config = {\n",
    "    'epochs': 100,  # Increased epochs\n",
    "    'callbacks': [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=7,  # Increased patience\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001  # Smaller improvement threshold\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,  # Increased patience\n",
    "            min_lr=1e-6,  # Lower minimum learning rate\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath='../data/models/checkpoint.model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(r'../data/logs')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=training_config['epochs'],\n",
    "    callbacks=training_config['callbacks']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], color='teal', label='loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join('..', 'data', 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(model_dir, 'uno_classifier.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_dir, 'uno_classifier.h5')\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('test_image.jpg')\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and scale the image\n",
    "resize = tf.image.resize(img_rgb, (256, 256))\n",
    "scaled_img = resize.numpy().astype(np.float32) / 255.0\n",
    "plt.imshow(scaled_img)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dimensions to match the model's input shape\n",
    "input_img = np.expand_dims(scaled_img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "yhat = model.predict(input_img)\n",
    "predicted_index = np.argmax(yhat, axis=1)[0]\n",
    "predicted_label = data.class_names[predicted_index]\n",
    "\n",
    "print(f\"Predicted Class Index: {predicted_index}\")\n",
    "print(f\"Predicted Class Name: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the prediction matches the expected class\n",
    "expected_index = 31  # 31st class (0-based index)\n",
    "expected_label = data.class_names[expected_index]\n",
    "\n",
    "if predicted_index == expected_index:\n",
    "    print(f\"The model correctly predicted the class: {predicted_label}\")\n",
    "else:\n",
    "    print(f\"The model predicted '{predicted_label}', but expected '{expected_label}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
